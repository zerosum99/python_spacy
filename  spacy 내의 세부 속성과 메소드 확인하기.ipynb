{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어를 처리할 수 있도록 실제 파싱 객체를 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "parser = English()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.en.English at 0x1070af5f8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.en.English at 0x10f9ea860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트 할 문장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Data\n",
    "multiSentence = \"There is an art, it says, or rather, a knack to flying.\" \\\n",
    "                 \"The knack lies in learning how to throw yourself at the ground and miss.\" \\\n",
    "                 \"In the beginning the Universe was created. This has made a lot of people \"\\\n",
    "                 \"very angry and been widely regarded as a bad move.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an art, it says, or rather, a knack to flying.The knack lies in learning how to throw yourself at the ground and miss.In the beginning the Universe was created. This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "print(multiSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc 클래스의 인스턴스 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsedData = parser(multiSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp_doc = nlp.tokenizer(multiSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_nlp = nlp(multiSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "\n",
    "doc_doc = Doc(nlp.vocab,multiSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc 클래스 내의 속성과 메소드 알아보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_by\n",
      "doc\n",
      "ents\n",
      "from_array\n",
      "from_bytes\n",
      "has_vector\n",
      "is_parsed\n",
      "is_tagged\n",
      "mem\n",
      "merge\n",
      "noun_chunks\n",
      "noun_chunks_iterator\n",
      "print_tree\n",
      "read_bytes\n",
      "sentiment\n",
      "sents\n",
      "similarity\n",
      "string\n",
      "tensor\n",
      "text\n",
      "text_with_ws\n",
      "to_array\n",
      "to_bytes\n",
      "user_data\n",
      "user_hooks\n",
      "user_span_hooks\n",
      "user_token_hooks\n",
      "vector\n",
      "vector_norm\n",
      "vocab\n"
     ]
    }
   ],
   "source": [
    "for i in dir(parsedData) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: 769 There\n",
      "lowercased: 608 there\n",
      "lemma: 608 there\n",
      "shape: 684 Xxxxx\n",
      "prefix: 568 T\n",
      "suffix: 609 ere\n",
      "log probability: -7.277902603149414\n",
      "Brown cluster id: 1918\n",
      "----------------------------------------\n",
      "original: 513 is\n",
      "lowercased: 513 is\n",
      "lemma: 536 be\n",
      "shape: 505 xx\n",
      "prefix: 509 i\n",
      "suffix: 513 is\n",
      "log probability: -4.3297648429870605\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 591 an\n",
      "lowercased: 591 an\n",
      "lemma: 591 an\n",
      "shape: 505 xx\n",
      "prefix: 506 a\n",
      "suffix: 591 an\n",
      "log probability: -5.953293800354004\n",
      "Brown cluster id: 3\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(parsedData):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 확인해 보기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an art, it says, or rather, a knack to flying.\n",
      "The knack lies in learning how to throw yourself at the ground and miss.\n",
      "In the beginning the Universe was created.\n",
      "This has made a lot of people very angry and been widely regarded as a bad move.\n"
     ]
    }
   ],
   "source": [
    "sents = []\n",
    "# the \"sents\" property returns spans\n",
    "# spans have indices into the original string\n",
    "# where each index value represents a token\n",
    "for span in parsedData.sents:\n",
    "    # go from the start to the end of each span, returning each token in the sentence\n",
    "    # combine each token using join()\n",
    "    sent = ''.join(parsedData[i].string for i in range(span.start, span.end)).strip()\n",
    "    sents.append(sent)\n",
    "\n",
    "for sentence in sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 제너레이터를 통해 문장을 하나씩 처리가 가능하다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator at 0x147324c48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData.sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(parsedData.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc 에서 하나의 문장을 찾으면 실제 Span 클래스의 인스턴스가 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'>\n",
      "There is an art, it says, or rather, a knack to flying.\n"
     ]
    }
   ],
   "source": [
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span  내의 속성과 메소드를 확인한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc\n",
      "end\n",
      "end_char\n",
      "ent_id\n",
      "ent_id_\n",
      "has_vector\n",
      "label\n",
      "label_\n",
      "lefts\n",
      "lemma_\n",
      "lower_\n",
      "merge\n",
      "noun_chunks\n",
      "orth_\n",
      "rights\n",
      "root\n",
      "sent\n",
      "sentiment\n",
      "similarity\n",
      "start\n",
      "start_char\n",
      "string\n",
      "subtree\n",
      "text\n",
      "text_with_ws\n",
      "upper_\n",
      "vector\n",
      "vector_norm\n"
     ]
    }
   ],
   "source": [
    "for i in dir(a) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 실제 원본 문장을 가진 데이터를 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There is an art, it says, or rather, a knack to flying.The knack lies in learning how to throw yourself at the ground and miss.In the beginning the Universe was created. This has made a lot of people very angry and been widely regarded as a bad move."
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하나의 문장을 가져왔으므로 이 시작과 끝에 대한 정보를 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.start, a.end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There is an art, it says, or rather, a knack to flying."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.doc[a.start:a.end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화가 되어있기에 인덱스로 토큰을 볼 수 있다.\n",
    "\n",
    "    실제 단어에 대한 출력을 한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There \n",
      "is \n",
      "an \n",
      "art\n",
      ", \n",
      "it \n",
      "says\n",
      ", \n",
      "or \n",
      "rather\n",
      ", \n",
      "a \n",
      "knack \n",
      "to \n",
      "flying\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in range(a.start, a.end) :\n",
    "    print(a[i].string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc로 되어있기 때문에 전체가 토근화 되어 있는 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n",
      "is\n",
      "an\n",
      "art\n",
      ",\n",
      "it\n",
      "says\n",
      ",\n",
      "or\n",
      "rather\n"
     ]
    }
   ],
   "source": [
    "for i in range(10) :\n",
    "    print(parsedData[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰에 필요한 속성들을 조회해서 출력하기 ..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The det boy [] []\n",
      "boy nsubj ran ['The'] ['with']\n",
      "with prep boy [] []\n",
      "the det dog [] []\n",
      "spotted amod dog [] []\n",
      "dog nsubj ran ['the', 'spotted'] []\n",
      "quickly advmod ran [] []\n",
      "ran ROOT ran ['boy', 'dog', 'quickly'] ['after', '.']\n",
      "after prep ran [] ['firetruck']\n",
      "the det firetruck [] []\n",
      "firetruck pobj after ['the'] []\n",
      ". punct ran [] []\n"
     ]
    }
   ],
   "source": [
    "example = \"The boy with the spotted dog quickly ran after the firetruck.\"\n",
    "parsedEx = parser(example)\n",
    "# shown as: original token, dependency tag, head word, left dependents, right dependents\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.dep_, token.head.orth_, [t.orth_ for t in token.lefts], [t.orth_ for t in token.rights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰을 처리해서 문장을 묶어서 다시 표기하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[There, is, an, art, ,, it, says, ,, or, rather, ,, a, knack, to, flying, .], [The, knack, lies, in, learning, how, to, throw, yourself, at, the, ground, and, miss, .], [In, the, beginning, the, Universe, was, created, .], [This, has, made, a, lot, of, people, very, angry, and, been, widely, regarded, as, a, bad, move, .]]\n"
     ]
    }
   ],
   "source": [
    "sent_ = []\n",
    "for span in parsedData.sents:\n",
    "    sent_.append([ parsedData[i] for i in range(span.start, span.end)])\n",
    "    \n",
    "print(sent_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰을 가지고 바로 품사를 붙이기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[There, is, an, art, ,, it, says, ,, or, rather, ,, a, knack, to, flying, .]\n",
      "There  :  ADV\n",
      "is  :  VERB\n",
      "an  :  DET\n",
      "art  :  NOUN\n",
      ",  :  PUNCT\n",
      "it  :  PRON\n",
      "says  :  VERB\n",
      ",  :  PUNCT\n",
      "or  :  CCONJ\n",
      "rather  :  ADV\n",
      ",  :  PUNCT\n",
      "a  :  DET\n",
      "knack  :  NOUN\n",
      "to  :  ADP\n",
      "flying  :  NOUN\n",
      ".  :  PUNCT\n"
     ]
    }
   ],
   "source": [
    "sent = []\n",
    "for span in parsedData.sents:\n",
    "    sent = [parsedData[i] for i in range(span.start, span.end)]\n",
    "    break\n",
    "\n",
    "print(sent)\n",
    "for token in sent:\n",
    "    print(token.orth_,\" : \", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엔티티 에 대한 네임을 확인한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "'s (not an entity)\n",
      "stocks (not an entity)\n",
      "dropped (not an entity)\n",
      "dramatically (not an entity)\n",
      "after (not an entity)\n",
      "the (not an entity)\n",
      "death (not an entity)\n",
      "of (not an entity)\n",
      "Steve PERSON\n",
      "Jobs PERSON\n",
      "in (not an entity)\n",
      "October DATE\n",
      ". (not an entity)\n",
      "-------------- entities only ---------------\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the named entities of this example:\n",
    "example = \"Apple's stocks dropped dramatically after the death of Steve Jobs in October.\"\n",
    "parsedEx = parser(example)\n",
    "for token in parsedEx:\n",
    "    print(token.orth_, token.ent_type_ if token.ent_type_ != \"\" else \"(not an entity)\")\n",
    "\n",
    "print(\"-------------- entities only ---------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 ORG Apple\n",
      "377 PERSON Steve Jobs\n",
      "387 DATE October\n"
     ]
    }
   ],
   "source": [
    "# if you just want the entities and nothing else, you can do access the parsed examples \"ents\" property like this:\n",
    "ents = list(parsedEx.ents)\n",
    "for entity in ents:\n",
    "    print(entity.label, entity.label_, ' '.join(t.orth_ for t in entity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰화된 단어, 품사, 단어 기본형 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol NOUN lol\n",
      "that ADJ that\n",
      "is VERB be\n",
      "rly ADV rly\n",
      "funny ADJ funny\n",
      ":) PUNCT :)\n",
      "This DET this\n",
      "is VERB be\n",
      "gr8 VERB gr8\n",
      "i PRON i\n",
      "rate VERB rate\n",
      "it PRON -PRON-\n",
      "8/8 NUM 8/8\n",
      "! PUNCT !\n",
      "! PUNCT !\n",
      "! PUNCT !\n"
     ]
    }
   ],
   "source": [
    "messyData = \"lol that is rly funny :) This is gr8 i rate it 8/8!!!\"\n",
    "parsedData = parser(messyData)\n",
    "for token in parsedData:\n",
    "    print(token.orth_, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy  모듈 설치하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-1.9.0.tar.gz (3.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.4MB 234kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: numpy>=1.7 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy)\n",
      "Collecting murmurhash<0.27,>=0.26 (from spacy)\n",
      "  Downloading murmurhash-0.26.4.tar.gz\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading cymem-1.31.2.tar.gz\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading preshed-1.0.0.tar.gz (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting thinc<6.6.0,>=6.5.0 (from spacy)\n",
      "  Downloading thinc-6.5.2.tar.gz (926kB)\n",
      "\u001b[K    100% |████████████████████████████████| 931kB 678kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: pip<10.0.0,>=9.0.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: six in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy)\n",
      "Collecting pathlib (from spacy)\n",
      "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting ujson>=1.35 (from spacy)\n",
      "  Downloading ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3,>=0.2 (from spacy)\n",
      "  Downloading dill-0.2.7.1.tar.gz (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 3.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting regex<2017.12.1,>=2017.4.1 (from spacy)\n",
      "  Downloading regex-2017.07.28.tar.gz (607kB)\n",
      "\u001b[K    100% |████████████████████████████████| 614kB 861kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2 (from spacy)\n",
      "  Downloading ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting wrapt (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading wrapt-1.10.11.tar.gz\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading tqdm-4.15.0-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: cytoolz<0.9,>=0.8 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Collecting termcolor (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Collecting idna<2.7,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 3.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting html5lib (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading html5lib-0.999999999-py2.py3-none-any.whl (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: wcwidth in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Collecting webencodings (from html5lib->ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl\n",
      "Collecting setuptools>=18.5 (from html5lib->ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading setuptools-36.2.7-py2.py3-none-any.whl (477kB)\n",
      "\u001b[K    100% |████████████████████████████████| 481kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: spacy, murmurhash, cymem, preshed, thinc, pathlib, ujson, dill, regex, ftfy, wrapt, termcolor\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/8c/a9/96/dba5bf6fdd55d8c04e9ffb1b6244137c36e8b33e03d3e66a9a\n",
      "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/88/99/5c/50281964395b8ce3942061f580c790f98432fe668dc9804779\n",
      "  Running setup.py bdist_wheel for cymem ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/4b/2a/0e/dce3ff7a6f0f916906ef978afe512a7b5aef8abfd9ba988acf\n",
      "  Running setup.py bdist_wheel for preshed ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/97/64/22/20fabf1f51039b799e64e46d0381b023cfdbe159c349d7c135\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/ea/1a/6d/0670caa3a983bebfaccf081d9bb092fbd7871d1c4eff8b2c70\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/2a/23/a5/d8803db5d631e9f391fe6defe982a238bf5483062eeb34e841\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/9e/9b/d0/df92653bb5b2664c15d8ee5b99e3f2eb08a034444db8922b2f\n",
      "  Running setup.py bdist_wheel for dill ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/e5/88/fe/7e290ce5bb39d531eb9bee5cf254ba1c3e3c7ba3339ce67bee\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/59/6f/80/c0f0aebad616d785f0178de88f9ae957991672856038d75a6c\n",
      "  Running setup.py bdist_wheel for ftfy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/ae/d7/4c/339066248431397227741c7fdc80ad85826188ee9b0c24b4c7\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/56/e1/0f/f7ccf1ed8ceaabccc2a93ce0481f73e589814cbbc439291345\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/de/f7/bf/1bcac7bf30549e6a4957382e2ecab04c88e513117207067b03\n",
      "Successfully built spacy murmurhash cymem preshed thinc pathlib ujson dill regex ftfy wrapt termcolor\n",
      "Installing collected packages: murmurhash, cymem, preshed, wrapt, tqdm, plac, dill, termcolor, pathlib, thinc, ujson, idna, requests, regex, webencodings, setuptools, html5lib, ftfy, spacy\n",
      "  Found existing installation: wrapt 1.10.10\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (wrapt) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling wrapt-1.10.10:\n",
      "      Successfully uninstalled wrapt-1.10.10\n",
      "  Found existing installation: idna 2.5\n",
      "    Uninstalling idna-2.5:\n",
      "      Successfully uninstalled idna-2.5\n",
      "  Found existing installation: requests 2.18.3\n",
      "    Uninstalling requests-2.18.3:\n",
      "      Successfully uninstalled requests-2.18.3\n",
      "  Found existing installation: setuptools 36.2.0\n",
      "    Uninstalling setuptools-36.2.0:\n",
      "      Successfully uninstalled setuptools-36.2.0\n",
      "  Found existing installation: html5lib 0.9999999\n",
      "    Uninstalling html5lib-0.9999999:\n",
      "      Successfully uninstalled html5lib-0.9999999\n",
      "Successfully installed cymem-1.31.2 dill-0.2.7.1 ftfy-4.4.3 html5lib-0.999999999 idna-2.6 murmurhash-0.26.4 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 regex-2017.7.28 requests-2.18.4 setuptools-36.2.7 spacy-1.9.0 termcolor-1.1.0 thinc-6.5.2 tqdm-4.15.0 ujson-1.35 webencodings-0.5.1 wrapt-1.10.11\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  영어처리 모듈 다운로드 하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 52.2MB 164kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.0.0,>=1.7.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pathlib in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: six in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wcwidth in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: html5lib in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: termcolor in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wrapt in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: webencodings in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-1.2.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "\n",
      "    /Users/dahlmoon/anaconda/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
      "    --> /Users/dahlmoon/anaconda/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 내부의 메소드 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_by\n",
      "doc\n",
      "ents\n",
      "from_array\n",
      "from_bytes\n",
      "has_vector\n",
      "is_parsed\n",
      "is_tagged\n",
      "mem\n",
      "merge\n",
      "noun_chunks\n",
      "noun_chunks_iterator\n",
      "print_tree\n",
      "read_bytes\n",
      "sentiment\n",
      "sents\n",
      "similarity\n",
      "string\n",
      "tensor\n",
      "text\n",
      "text_with_ws\n",
      "to_array\n",
      "to_bytes\n",
      "user_data\n",
      "user_hooks\n",
      "user_span_hooks\n",
      "user_token_hooks\n",
      "vector\n",
      "vector_norm\n",
      "vocab\n"
     ]
    }
   ],
   "source": [
    "for i in dir(spacy.tokens.doc.Doc) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy 내의 언어 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.de.German'>\n",
      "<class 'spacy.en.English'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "print(spacy.util.get_lang_class('de'))\n",
    "print(spacy.util.get_lang_class('en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 자연어 처리를 위해 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한문장에 대해 자연어 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this's spacy tokenize test\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"this's spacy tokenize test\")\n",
    "\n",
    "print(doc1)\n",
    "print(type(doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 분석한 내부의 단어를 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'DET'), (\"'s\", 'VERB'), ('spacy', 'NOUN'), ('tokenize', 'NOUN'), ('test', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print([(w.text, w.pos_) for w in doc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for w in doc1 :\n",
    "    print(type(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy tokenize test\n"
     ]
    }
   ],
   "source": [
    "for i in doc1.noun_chunks :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트를 받아서 문장으로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'> this is spacy sentence tokenize test.\n",
      "<class 'spacy.tokens.span.Span'> this is second sent!\n",
      "<class 'spacy.tokens.span.Span'> is this the third sent? final test.\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"this is spacy sentence tokenize test. this is second sent! is this the third sent? final test.\")\n",
    "\n",
    "for sent in doc2.sents:\n",
    "    print(type(sent), sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 받아서 단어로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this : DET\n",
      "is : VERB\n",
      "spacy : NOUN\n",
      "sentence : NOUN\n",
      "tokenize : NOUN\n",
      "test : NOUN\n",
      ". : PUNCT\n",
      "this : DET\n",
      "is : VERB\n",
      "second : ADJ\n",
      "sent : VERB\n",
      "! : PUNCT\n",
      "is : VERB\n",
      "this : DET\n",
      "the : DET\n",
      "third : ADJ\n",
      "sent : VERB\n",
      "? : PUNCT\n",
      "final : ADJ\n",
      "test : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sent in doc2.sents:\n",
    "    for w in sent :\n",
    "        print(w.text,\":\", w.pos_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'INTJ'), (',', 'PUNCT'), ('spacy', 'NOUN'), ('!', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy                           # See \"Installing spaCy\"\n",
    "nlp = spacy.load('en')                 # You are here.\n",
    "doc = nlp('Hello, spacy!')            # See \"Using the pipeline\"\n",
    "print([(w.text, w.pos_) for w in doc]) # See \"Doc, Span and Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ['One document.', '...', 'Lots of documents']\n",
    "# .pipe streams input, and produces streaming output\n",
    "iter_texts = (texts[i % 3] for i in range(100))\n",
    "for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=4)):\n",
    "    assert doc.is_parsed\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "One document.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3df29683449a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Hello, world.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token = doc[0]\n",
    "\n",
    "print(token)\n",
    "\n",
    "sentence = next(doc.sents)\n",
    "print(sentence)\n",
    "assert token is sentence[0]\n",
    "assert sentence.text == 'Hello, world.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사 태깅하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They 757862 -PRON- 479 PRP 93 PRON\n",
      "told 971 tell 489 VBD 98 VERB\n",
      "us 757862 -PRON- 479 PRP 93 PRON\n",
      "to 504 to 486 TO 92 PART\n",
      "duck 7797 duck 474 NN 90 NOUN\n",
      ". 453 . 453 . 95 PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'They told us to duck.')\n",
    "for word in doc:\n",
    "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_type', 'ent_type_', 'has_repvec', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ancestor_of', 'is_ascii', 'is_bracket', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_space', 'is_stop', 'is_title', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'repvec', 'right_edge', 'rights', 'sentiment', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "for word in doc :\n",
    "    print(dir(word))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰된 단어를 가지고 엔티티 인식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London 3 GPE\n",
      "is 2 \n"
     ]
    }
   ],
   "source": [
    "doc = nlp('London is a big city in the United Kingdom.')\n",
    "print(doc[0].text, doc[0].ent_iob, doc[0].ent_type_)\n",
    "# (u'London', 2, u'GPE')\n",
    "print(doc[1].text, doc[1].ent_iob, doc[1].ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> London\n"
     ]
    }
   ],
   "source": [
    "print(type(doc[0]),doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spacy  모듈 설치하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-1.9.0.tar.gz (3.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.4MB 234kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: numpy>=1.7 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy)\n",
      "Collecting murmurhash<0.27,>=0.26 (from spacy)\n",
      "  Downloading murmurhash-0.26.4.tar.gz\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading cymem-1.31.2.tar.gz\n",
      "Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading preshed-1.0.0.tar.gz (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.8MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting thinc<6.6.0,>=6.5.0 (from spacy)\n",
      "  Downloading thinc-6.5.2.tar.gz (926kB)\n",
      "\u001b[K    100% |████████████████████████████████| 931kB 678kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: pip<10.0.0,>=9.0.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: six in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy)\n",
      "Collecting pathlib (from spacy)\n",
      "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 2.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting ujson>=1.35 (from spacy)\n",
      "  Downloading ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3,>=0.2 (from spacy)\n",
      "  Downloading dill-0.2.7.1.tar.gz (64kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 3.5MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 1.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting regex<2017.12.1,>=2017.4.1 (from spacy)\n",
      "  Downloading regex-2017.07.28.tar.gz (607kB)\n",
      "\u001b[K    100% |████████████████████████████████| 614kB 861kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2 (from spacy)\n",
      "  Downloading ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.3MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting wrapt (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading wrapt-1.10.11.tar.gz\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading tqdm-4.15.0-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 3.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: cytoolz<0.9,>=0.8 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Collecting termcolor (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Collecting idna<2.7,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 3.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: urllib3<1.23,>=1.21.1 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: chardet<3.1.0,>=3.0.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: certifi>=2017.4.17 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting html5lib (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading html5lib-0.999999999-py2.py3-none-any.whl (112kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 1.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: wcwidth in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Collecting webencodings (from html5lib->ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl\n",
      "Collecting setuptools>=18.5 (from html5lib->ftfy<5.0.0,>=4.4.2->spacy)\n",
      "  Downloading setuptools-36.2.7-py2.py3-none-any.whl (477kB)\n",
      "\u001b[K    100% |████████████████████████████████| 481kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: spacy, murmurhash, cymem, preshed, thinc, pathlib, ujson, dill, regex, ftfy, wrapt, termcolor\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/8c/a9/96/dba5bf6fdd55d8c04e9ffb1b6244137c36e8b33e03d3e66a9a\n",
      "  Running setup.py bdist_wheel for murmurhash ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/88/99/5c/50281964395b8ce3942061f580c790f98432fe668dc9804779\n",
      "  Running setup.py bdist_wheel for cymem ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/4b/2a/0e/dce3ff7a6f0f916906ef978afe512a7b5aef8abfd9ba988acf\n",
      "  Running setup.py bdist_wheel for preshed ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/97/64/22/20fabf1f51039b799e64e46d0381b023cfdbe159c349d7c135\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/ea/1a/6d/0670caa3a983bebfaccf081d9bb092fbd7871d1c4eff8b2c70\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/2a/23/a5/d8803db5d631e9f391fe6defe982a238bf5483062eeb34e841\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/9e/9b/d0/df92653bb5b2664c15d8ee5b99e3f2eb08a034444db8922b2f\n",
      "  Running setup.py bdist_wheel for dill ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/e5/88/fe/7e290ce5bb39d531eb9bee5cf254ba1c3e3c7ba3339ce67bee\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/59/6f/80/c0f0aebad616d785f0178de88f9ae957991672856038d75a6c\n",
      "  Running setup.py bdist_wheel for ftfy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/ae/d7/4c/339066248431397227741c7fdc80ad85826188ee9b0c24b4c7\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/56/e1/0f/f7ccf1ed8ceaabccc2a93ce0481f73e589814cbbc439291345\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/dahlmoon/Library/Caches/pip/wheels/de/f7/bf/1bcac7bf30549e6a4957382e2ecab04c88e513117207067b03\n",
      "Successfully built spacy murmurhash cymem preshed thinc pathlib ujson dill regex ftfy wrapt termcolor\n",
      "Installing collected packages: murmurhash, cymem, preshed, wrapt, tqdm, plac, dill, termcolor, pathlib, thinc, ujson, idna, requests, regex, webencodings, setuptools, html5lib, ftfy, spacy\n",
      "  Found existing installation: wrapt 1.10.10\n",
      "\u001b[31m    DEPRECATION: Uninstalling a distutils installed project (wrapt) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\u001b[0m\n",
      "    Uninstalling wrapt-1.10.10:\n",
      "      Successfully uninstalled wrapt-1.10.10\n",
      "  Found existing installation: idna 2.5\n",
      "    Uninstalling idna-2.5:\n",
      "      Successfully uninstalled idna-2.5\n",
      "  Found existing installation: requests 2.18.3\n",
      "    Uninstalling requests-2.18.3:\n",
      "      Successfully uninstalled requests-2.18.3\n",
      "  Found existing installation: setuptools 36.2.0\n",
      "    Uninstalling setuptools-36.2.0:\n",
      "      Successfully uninstalled setuptools-36.2.0\n",
      "  Found existing installation: html5lib 0.9999999\n",
      "    Uninstalling html5lib-0.9999999:\n",
      "      Successfully uninstalled html5lib-0.9999999\n",
      "Successfully installed cymem-1.31.2 dill-0.2.7.1 ftfy-4.4.3 html5lib-0.999999999 idna-2.6 murmurhash-0.26.4 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 regex-2017.7.28 requests-2.18.4 setuptools-36.2.7 spacy-1.9.0 termcolor-1.1.0 thinc-6.5.2 tqdm-4.15.0 ujson-1.35 webencodings-0.5.1 wrapt-1.10.11\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  영어처리 모듈 다운로드 하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 52.2MB 164kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.0.0,>=1.7.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pathlib in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: six in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wcwidth in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: html5lib in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: termcolor in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wrapt in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: webencodings in /Users/dahlmoon/anaconda/lib/python3.6/site-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-1.2.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "\n",
      "    /Users/dahlmoon/anaconda/lib/python3.6/site-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
      "    --> /Users/dahlmoon/anaconda/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 내부의 메소드 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_by\n",
      "doc\n",
      "ents\n",
      "from_array\n",
      "from_bytes\n",
      "has_vector\n",
      "is_parsed\n",
      "is_tagged\n",
      "mem\n",
      "merge\n",
      "noun_chunks\n",
      "noun_chunks_iterator\n",
      "print_tree\n",
      "read_bytes\n",
      "sentiment\n",
      "sents\n",
      "similarity\n",
      "string\n",
      "tensor\n",
      "text\n",
      "text_with_ws\n",
      "to_array\n",
      "to_bytes\n",
      "user_data\n",
      "user_hooks\n",
      "user_span_hooks\n",
      "user_token_hooks\n",
      "vector\n",
      "vector_norm\n",
      "vocab\n"
     ]
    }
   ],
   "source": [
    "for i in dir(spacy.tokens.doc.Doc) :\n",
    "    if not i.startswith(\"_\") :\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spacy 내의 언어 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.de.German'>\n",
      "<class 'spacy.en.English'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "print(spacy.util.get_lang_class('de'))\n",
    "print(spacy.util.get_lang_class('en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 자연어 처리를 위해 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한문장에 대해 자연어 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this's spacy tokenize test\n",
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"this's spacy tokenize test\")\n",
    "\n",
    "print(doc1)\n",
    "print(type(doc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 분석한 내부의 단어를 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'DET'), (\"'s\", 'VERB'), ('spacy', 'NOUN'), ('tokenize', 'NOUN'), ('test', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print([(w.text, w.pos_) for w in doc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for w in doc1 :\n",
    "    print(type(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy tokenize test\n"
     ]
    }
   ],
   "source": [
    "for i in doc1.noun_chunks :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n",
      "'s\n",
      "spacy\n",
      "tokenize\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "print(doc1[0])\n",
    "print(doc1[1])\n",
    "print(doc1[2])\n",
    "print(doc1[3])\n",
    "print(doc1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1[2].similarity(doc1[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc7 = nlp(\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "\n",
    "apples = doc7[0]\n",
    "oranges = doc7[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apples.similarity(oranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트를 받아서 문장으로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.span.Span'> this is spacy sentence tokenize test.\n",
      "<class 'spacy.tokens.span.Span'> this is second sent!\n",
      "<class 'spacy.tokens.span.Span'> is this the third sent? final test.\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"this is spacy sentence tokenize test. this is second sent! is this the third sent? final test.\")\n",
    "\n",
    "for sent in doc2.sents:\n",
    "    print(type(sent), sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문장을 받아서 단어로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this : DET\n",
      "is : VERB\n",
      "spacy : NOUN\n",
      "sentence : NOUN\n",
      "tokenize : NOUN\n",
      "test : NOUN\n",
      ". : PUNCT\n",
      "this : DET\n",
      "is : VERB\n",
      "second : ADJ\n",
      "sent : VERB\n",
      "! : PUNCT\n",
      "is : VERB\n",
      "this : DET\n",
      "the : DET\n",
      "third : ADJ\n",
      "sent : VERB\n",
      "? : PUNCT\n",
      "final : ADJ\n",
      "test : NOUN\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sent in doc2.sents:\n",
    "    for w in sent :\n",
    "        print(w.text,\":\", w.pos_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'INTJ'), (',', 'PUNCT'), ('spacy', 'NOUN'), ('!', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "import spacy                           # See \"Installing spaCy\"\n",
    "nlp = spacy.load('en')                 # You are here.\n",
    "doc = nlp('Hello, spacy!')            # See \"Using the pipeline\"\n",
    "print([(w.text, w.pos_) for w in doc]) # See \"Doc, Span and Token\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ['One document.', '...', 'Lots of documents']\n",
    "# .pipe streams input, and produces streaming output\n",
    "iter_texts = (texts[i % 3] for i in range(100))\n",
    "for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=4)):\n",
    "    assert doc.is_parsed\n",
    "    if i == 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One\n",
      "One document.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3df29683449a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Hello, world.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "token = doc[0]\n",
    "\n",
    "print(token)\n",
    "\n",
    "sentence = next(doc.sents)\n",
    "print(sentence)\n",
    "assert token is sentence[0]\n",
    "assert sentence.text == 'Hello, world.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사 태깅하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They 757862 -PRON- 479 PRP 93 PRON\n",
      "told 971 tell 489 VBD 98 VERB\n",
      "us 757862 -PRON- 479 PRP 93 PRON\n",
      "to 504 to 486 TO 92 PART\n",
      "duck 7797 duck 474 NN 90 NOUN\n",
      ". 453 . 453 . 95 PUNCT\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'They told us to duck.')\n",
    "for word in doc:\n",
    "    print(word.text, word.lemma, word.lemma_, word.tag, word.tag_, word.pos, word.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_type', 'ent_type_', 'has_repvec', 'has_vector', 'head', 'i', 'idx', 'is_alpha', 'is_ancestor', 'is_ancestor_of', 'is_ascii', 'is_bracket', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_space', 'is_stop', 'is_title', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'repvec', 'right_edge', 'rights', 'sentiment', 'shape', 'shape_', 'similarity', 'string', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "for word in doc :\n",
    "    print(dir(word))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토큰된 단어를 가지고 엔티티 인식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London 3 GPE\n",
      "is 2 \n"
     ]
    }
   ],
   "source": [
    "doc = nlp('London is a big city in the United Kingdom.')\n",
    "print(doc[0].text, doc[0].ent_iob, doc[0].ent_type_)\n",
    "# (u'London', 2, u'GPE')\n",
    "print(doc[1].text, doc[1].ent_iob, doc[1].ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> London\n"
     ]
    }
   ],
   "source": [
    "print(type(doc[0]),doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엔티티에 대한 세부 명칭 \n",
    "\n",
    "    Named Entity Recognizer (NER) Test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rami Eid 377 PERSON\n",
      "Stony Brook University 380 ORG\n"
     ]
    }
   ],
   "source": [
    "doc5 = nlp(\"Rami Eid is studying at Stony Brook University in New York . Hello!!\")\n",
    "\n",
    "for ent in doc5.ents:\n",
    "    print(ent, ent.label, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dahl Moon 377 PERSON\n",
      "Soongsil University 380 ORG\n",
      "New York 381 GPE\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(\"Dahl Moon is studying at Soongsil University.  New York . \")\n",
    "\n",
    "for ent in doc6.ents:\n",
    "    print(ent, ent.label, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "apples, and_, oranges = nlp(u'apples and oranges')\n",
    "print(apples.vector.shape)\n",
    "# (1,)\n",
    "apples.similarity(oranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.63340002,  0.18980999, -0.53544003, -0.52657998, -0.30001   ,\n",
       "        0.30559   , -0.49303001,  0.14636   ,  0.012273  ,  0.96802002,\n",
       "        0.0040354 ,  0.25233999, -0.29864001, -0.014646  , -0.24905001,\n",
       "       -0.67124999, -0.053366  ,  0.59425998, -0.068034  ,  0.10315   ,\n",
       "        0.66759002,  0.024617  , -0.37548   ,  0.52556998,  0.054449  ,\n",
       "       -0.36748001, -0.28013   ,  0.090898  , -0.025687  , -0.59469998,\n",
       "       -0.24269   ,  0.28602999,  0.68599999,  0.29736999,  0.30421999,\n",
       "        0.69032001,  0.042784  ,  0.023701  , -0.57165003,  0.70581001,\n",
       "       -0.20813   , -0.03204   , -0.12494   , -0.42932999,  0.31270999,\n",
       "        0.30351999,  0.09421   , -0.15493   ,  0.071356  ,  0.15022001,\n",
       "       -0.41791999,  0.066394  , -0.034546  , -0.45772001,  0.57177001,\n",
       "       -0.82754999, -0.27884999,  0.71801001, -0.12425   ,  0.18550999,\n",
       "        0.41341999, -0.53996998,  0.55864   , -0.015805  , -0.1074    ,\n",
       "       -0.29980999, -0.17271   ,  0.27066001,  0.043996  ,  0.60106999,\n",
       "       -0.35299999,  0.68309999,  0.20703   ,  0.12068   ,  0.24852   ,\n",
       "       -0.15605   ,  0.25812   ,  0.007004  , -0.10741   , -0.097053  ,\n",
       "        0.085628  ,  0.096307  ,  0.20857   , -0.23338   , -0.077905  ,\n",
       "       -0.030906  ,  1.04939997,  0.55368   , -0.10703   ,  0.052234  ,\n",
       "        0.43406999, -0.13925999,  0.38115001,  0.021104  , -0.40922001,\n",
       "        0.35971999, -0.28898001,  0.30618   ,  0.060807  , -0.023517  ,\n",
       "        0.58192998, -0.3098    ,  0.21013001, -0.15557   , -0.56913   ,\n",
       "       -1.13639998,  0.36598   , -0.032666  ,  1.19260001,  0.12825   ,\n",
       "       -0.090486  , -0.47964999, -0.61163998, -0.16484   , -0.41134   ,\n",
       "        0.19925   ,  0.059183  , -0.20841999,  0.45223001,  0.27697   ,\n",
       "       -0.20745   ,  0.025404  , -0.28874001,  0.040478  , -0.22274999,\n",
       "       -0.43323001,  0.76956999, -0.054327  , -0.35213   , -0.30842   ,\n",
       "       -0.48791   , -0.35563999,  0.19813   , -0.094767  , -0.50918001,\n",
       "        0.18763   , -0.087555  ,  0.37709001, -0.1322    , -0.096913  ,\n",
       "       -1.9102    ,  0.55813003,  0.27390999, -0.077744  , -0.43933001,\n",
       "       -0.10367   , -0.24408001,  0.41869   ,  0.11659   ,  0.27454001,\n",
       "        0.81020999, -0.11006   ,  0.43131   ,  0.29095   , -0.49548   ,\n",
       "       -0.31957999, -0.072506  ,  0.020286  ,  0.21789999,  0.22032   ,\n",
       "       -0.29212001,  0.75638998,  0.13598   ,  0.019736  , -0.83104002,\n",
       "        0.22836   , -0.28669   , -1.05289996,  0.052771  ,  0.41266   ,\n",
       "        0.50149   ,  0.5323    ,  0.51573002, -0.31806001, -0.4619    ,\n",
       "        0.21739   , -0.43584001, -0.41382   ,  0.042237  , -0.57178998,\n",
       "        0.067623  , -0.27853999,  0.090044  ,  0.20633   ,  0.024678  ,\n",
       "       -0.57703   , -0.020183  , -0.53147   , -0.37548   , -0.12795   ,\n",
       "       -0.093662  , -0.0061183 ,  0.20220999, -0.62295997, -0.29745999,\n",
       "        0.26934999,  0.59008998, -0.50382   , -0.69757003,  0.20157   ,\n",
       "       -0.33592001, -0.45765999,  0.14060999,  0.22982   ,  0.044046  ,\n",
       "        0.26385999,  0.02942   ,  0.34095001,  1.14960003, -0.15555   ,\n",
       "       -0.064071  ,  0.30138999,  0.024211  , -0.63515002, -0.73347002,\n",
       "       -0.10346   , -0.22637001, -0.056392  , -0.16734999, -0.097331  ,\n",
       "       -0.19205999, -0.18866   ,  0.15116   , -0.038048  ,  0.70204997,\n",
       "        0.11586   , -0.14813   ,  0.0095166 , -0.33803999, -0.10158   ,\n",
       "       -0.23829   , -0.22758999,  0.092504  , -0.29839   , -0.39721   ,\n",
       "        0.26091999,  0.34593999, -0.47396001, -0.25725001, -0.19257   ,\n",
       "       -0.53070998,  0.1692    , -0.47251999, -0.17332999, -0.40505001,\n",
       "        0.046446  , -0.04473   ,  0.33555001, -0.5693    ,  0.31591001,\n",
       "       -0.21167   , -0.31298   , -0.45923001, -0.083091  ,  0.086822  ,\n",
       "        0.01264   ,  0.43779001,  0.12650999,  0.30156001,  0.022061  ,\n",
       "        0.26549   , -0.29455   , -0.14838   ,  0.033692  , -0.37345999,\n",
       "       -0.075343  , -0.56497997, -0.24207   , -0.69351   , -0.20276999,\n",
       "       -0.0081185 ,  0.030971  ,  0.53614998, -0.16613001, -0.84087002,\n",
       "        0.74660999,  0.029132  ,  0.46935999, -0.49755001,  0.40954   ,\n",
       "       -0.022558  ,  0.21496999, -0.049528  , -0.039799  ,  0.46165001,\n",
       "        0.26456001,  0.32984999, -0.04219   , -0.099599  , -0.17312001,\n",
       "       -0.47600001, -0.019048  , -0.41887999, -0.2685    , -0.65280998,\n",
       "        0.068773  , -0.23881   , -1.17840004,  0.25503999,  0.61171001], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apples.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
